{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing & Organizing NCEI/NOAA Data\n",
    "------------------------------------------------------------------------------\n",
    "\n",
    "For this tutorial we will be using some U.S. Hourly Precipitation Data, published by NOAA:\n",
    "\n",
    "**Citation:**\n",
    "\n",
    "> National Climatic Data Center, NESDIS, NOAA, U.S. Department of Commerce (2016). _U.S. Hourly Precipitation Data [dataset]_. NCEI DSI 3240_01 \n",
    "\n",
    "> National Climatic Data Center, NESDIS, NOAA, U.S. Department of Commerce (2016). _U.S. Hourly Precipitation Data [dataset]_. NCEI DSI 3240_02\n",
    "\n",
    "From the webpage at <https://data.nodc.noaa.gov/cgi-bin/iso?id=gov.noaa.ncdc:C00313>, we see that data can be downloaded via the FTP server at <ftp://ftp.ncdc.noaa.gov/pub/data/hourly_precip-3240/>.\n",
    "\n",
    "For our purposes, we are only going to download a small subset of the available data.\n",
    "\n",
    "Our objective is to use shell commands to download, organize, and prepare the data for processing.\n",
    "\n",
    "We will use the `wget` command line utility to download the data - if for any reason you are unable to install or get `wget` working, the data are available for download from <https://unmm-my.sharepoint.com/:u:/g/personal/jwheel01_unm_edu/EQ4cpOBAiHVEhSUdkfQFSAcBgEnWUViyqdIXKvwglz6bWQ?e=flQL41>.\n",
    "\n",
    "### Create the Project Directory\n",
    "\n",
    "First we need to create a working directory for the project, using our `home` directory (Mac and Linux) or our user directory (Windows).\n",
    "\n",
    "Within the shell, take a moment to determine which directory you are in, change to your `home` directory, and verify that you have done so.\n",
    "\n",
    "Now use the `mkdir` command to create a directory named *precipitation_data*. Note that, similar to `cd`, the `mkdir` command does not print any output. Once done, change into this directory.\n",
    "\n",
    "```\n",
    "mkdir precipitation_data\n",
    "```\n",
    "\n",
    "### Download Data\n",
    "\n",
    "One way to get the data we need is to use a web browser to download the files from the FTP site linked above. This is straight forward but presents two problems:\n",
    "\n",
    "1. Each directory contains multiple tarfiles of data - unless we use a bulk download manager, the more data we need, the longer it will take to download.\n",
    "2. Unless we specify the download directory each time, the files will be saved to some other location on our computer and we will have to move them to our *precipitation_data* directory.\n",
    "\n",
    "This is exactly the type of repetitive and error prone task that benefits from batch processing using shell commands. We're going to use a command line utility called `wget` to download the complete contents of each directory. First, we can use the standard `--help` flag to check the usage and parameters.\n",
    "\n",
    "```\n",
    "wget --help\n",
    "```\n",
    "\n",
    "More info on `wget` is available at <https://www.gnu.org/software/wget/>.\n",
    "\n",
    "Looking at the help info, we see there are a couple of ways we can simplify our download process. One option is to use the `-r` flag, which recursively follows the links found within the specified URL. In our case, this will result in downloading all of the files in whichever FTP directory we specify when we run the command:\n",
    "\n",
    "```\n",
    "wget -r ftp://ftp.ncdc.noaa.gov/pub/data/hourly_precip-3240/66/\n",
    "```\n",
    "\n",
    "**Quick check:** What's one way we can re-run the command above for the other two directories without having to paste in or type the other two URLs?\n",
    "\n",
    "This strategy works fine for a small handful of URLs, but itself becomes repetitive (and error prone) if we run the command manually to download the contents of many more directories. Referring back to the help info, another option is to use the `-i` flag to specify an input file that contains a list of the URLs we want to harvest from.\n",
    "\n",
    "There are multiple ways to create the file we need for this, including using a text editor in the shell itself. For today, we will instead create the file in the shell using the `touch` command and then use our operating system's default text editor to add some URLs.\n",
    "\n",
    "```\n",
    "touch --help\n",
    "```\n",
    "\n",
    "After consulting the help info, let's create an empty file called `urls.txt`:\n",
    "\n",
    "```\n",
    "touch urls.txt\n",
    "```\n",
    "\n",
    "`touch` is another command that doesn't print any output. How can we verify our file was created?\n",
    "\n",
    "Open `urls.xt` with a text editor and paste in the following lines:\n",
    "\n",
    "```\n",
    "ftp://ftp.ncdc.noaa.gov/pub/data/hourly_precip-3240/51/\n",
    "ftp://ftp.ncdc.noaa.gov/pub/data/hourly_precip-3240/50/\n",
    "ftp://ftp.ncdc.noaa.gov/pub/data/hourly_precip-3240/48/\n",
    "```\n",
    "\n",
    "Keep in mind that we can add as many URLs as we like and will still only have to run a single `wget` command. Even better, this file becomes documentation that we can use to double check or replicate/reproduce our work.\n",
    "\n",
    "We can now use `wget` with the `-r` and `-i` flags to download the contents of the FTP URLs listed in `urls.txt`.\n",
    "\n",
    "```\n",
    "wget -r -i urls.txt\n",
    "```\n",
    "\n",
    "The final lines of the output gives us some idea of the effort we saved:\n",
    "\n",
    "```\n",
    "FINISHED --2019-02-05 20:32:07--\n",
    "Total wall clock time: 39s\n",
    "Downloaded: 42 files, 11M in 21s (521 KB/s)\n",
    "```\n",
    "\n",
    "#### Exercise\n",
    "\n",
    "Documentation for the data are available from the FTP site linked above. Using shell commands:\n",
    "\n",
    "1. Create a *documentation* directory in the *precipitation_data* directory. Change into this directory.\n",
    "2. Use one of the above methods to download the following two files into the *documentation* directory:\n",
    "    * dsi3240.pdf\n",
    "    * readme.txt\n",
    "    \n",
    "### Access the Data: Unzip and Move Files\n",
    "\n",
    "So far so good, but now we have 80+ tarfiles that have to be decompressed before we can use them! As with downloading, this is something we could do manually but it would take longer than necessary.\n",
    "\n",
    "Additionally, if we uncompress the files into the same directories as the compressed tarfiles we downloaded, our workspace becomes cluttered. So to keep our organization simple, we also want to move the decompressed files to a different directory.\n",
    "\n",
    "We will do one manually just to demonstrate.\n",
    "\n",
    "We will also use the `tar` command to go through the process for a single file, but first let's create a directory to store our uncompressed data. Using one of the methods we discussed for specifying a relative or absolute path, create a new directory *uncompressed_data* in the *precipitation_data* directory.\n",
    "\n",
    "Here's the steps for untarring a single file:\n",
    "\n",
    "```\n",
    "tar --help\n",
    "\n",
    "tar -C ~/Documents/work/precipitation_data/uncompressed_data/ -xf 3240_48_1948-1998.tar.Z\n",
    "```\n",
    "\n",
    "Considering how many files we have, this is not much of an improvement over using a GUI to uncompress everything manually. We could try a wildcard, like `tar -C ~/Documents/work/precipitation_data/uncompressed_data/ -xf *.tar.Z`, but unfortunately `tar` doesn't allow such use.\n",
    "\n",
    "#### Using `For` Loops\n",
    "\n",
    "As an interpreter, the shell allows us to do more than execute single commands. For our current purposes, we can use **for**, **while**, and **until** loops to iterate a command over a group or series (in our case, a series of files). We won't be using conditional logic today, but conditional (**if**) statements are also available for processing shell commands.\n",
    "\n",
    "The basic syntax of a **for** loop is\n",
    "\n",
    "```\n",
    "for i in series; do\n",
    "    some command $i\n",
    "done\n",
    "```\n",
    "Using the `tar` command from above, we can use the following loop to untar all of the files in a directory.\n",
    "\n",
    "```\n",
    "for t in *.tar; do\n",
    "    tar -C ~/Documents/work/precipitation_data/uncompressed_data/ xf \"$f\";\n",
    "done\n",
    "```\n",
    "\n",
    "A further refinement to make the loop easier to read is to set the path to the *uncompressed_data* directory as a variable. Using \"dataDir\" as our variable name:\n",
    "\n",
    "```\n",
    "dataDir=~/Documents/work/precipitation_data/uncompressed_data/ # Note - no spaces in the variable declaration!\n",
    "for t in *.tar.Z; do\n",
    "    tar -C $dataDir -xf $t;\n",
    "done\n",
    "```\n",
    "\n",
    "This works! But we have several directories of tarfiles, and again we would find ourselves repeating the process for every directory. No problem with a handful, but if we had downloaded all of the data from the FTP site it would still be a time consuming process.\n",
    "\n",
    "Fortunately, loops can be nested - we can run a loop in a loop - which gives use the possibility to untar ALL of the downloaded files at once. Making sure to run the following command from the parent directory containing all of the downloaded data directories:\n",
    "\n",
    "```\n",
    "dataDir=~/Documents/work/precipitation_data/uncompressed_data/\n",
    "for d in */; do\n",
    "    for t in $d/*.tar.Z; do\n",
    "        tar -C $dataDir -xf $t;\n",
    "    done\n",
    "done\n",
    "```\n",
    "\n",
    "That's it - all of the data have been uncompressed into the specified directory.\n",
    "\n",
    "Before moving on, it's possible we may want to download additional data at some point. Rather than go through the process of rewriting the above loop, we can save it as a script which can be executed at need. All that's needed is to add one line:\n",
    "\n",
    "```\n",
    "#!/bin/bash\n",
    "dataDir=~/Documents/work/precipitation_data/uncompressed_data/\n",
    "for d in */; do\n",
    "    for t in $d/*.tar.Z; do\n",
    "        tar -C $dataDir -xf $t;\n",
    "    done\n",
    "done\n",
    "```\n",
    "\n",
    "#### Exercise\n",
    "\n",
    "Using the `touch` command and a text editor, create a file named `untar_precip_data.sh` and copy the above script into it. Be sure to create the file in the directory which contains all of the download directories.\n",
    "\n",
    "Before we can run a shell script we have to make it executable by using the `chmod` command's capability to set the *executable* permission for the user, group, or others for the script file.\n",
    "\n",
    "```\n",
    "chmod --help\n",
    "\n",
    "chmod u+x untar_precip_data.sh\n",
    "```\n",
    "\n",
    "As needed, the script can be run as\n",
    "\n",
    "```\n",
    "./untar_precip_data.sh\n",
    "```\n",
    "\n",
    "Note that in many cases we need to specify the path to the script - here we have used the shorthand for the directory that contains the script.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
